---
title: "Deep Learning for Finance with TensorFlow"
subtitle: "R + Python"
author: 
date: 
output: 
   html_document:
       theme: flatly
       toc: true
       toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo    = TRUE,
    message = FALSE,
    warning = FALSE
)
```



# Libraries

```{r}
# API
library(Quandl)
library(config)
# Finance
library(tidyquant)

# Time Series

library(timetk)    
library(lubridate)

# Preprocessing
library(recipes)

# Accuracy
library(yardstick)

# Core
library(tidyverse)
library(janitor)
```

# Setup R's Python interface (Conda py3.6 Environment)

## Preparation 



- Install the Anaconda Distribution
- Get Python Scikit Learn Setup in R
- Make "py3.6" conda environment with `TensorFlow`, `scikit-learn`, `numpy`, `pandas` and `matplotlib`.

```{r}
# library(tensorflow)
# tensorflow::install_tensorflow(
#     method               = "conda",
#     version              = "default", # Installs TF 2.0.0 (as of May 15, 2020)
#     envname              = "py3.6",
#     conda_python_version = "3.6",
#     extra_packages       = c("matplotlib", "numpy", "pandas", "scikit-learn")
# )
```


## Python Setup

```{r}
library(reticulate)

# Replace this with your conda environment containing TensorFlow
use_condaenv("C:/Users/DELL/anaconda3", required = TRUE)
py_config()
```


## Special Setup Notes 

### Scikit Learn

If you plan on using Scikit Learn in your `py3.6` conda environrment, downgrade `scipy=1.4.0`.

```{python, eval=F}
from sklearn.preprocessing import StandardScaler, Normalizer
```

__Import Error:__

```
>>> from sklearn.preprocessing import StandardScaler, Normalizer
ImportError: dlopen(/Users/mdancho/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/scipy/special/_ufuncs.cpython-36m-darwin.so, 2): Symbol not found: _main
  Referenced from: /Users/mdancho/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/scipy/special/_ufuncs.cpython-36m-darwin.so
  Expected in: flat namespace
 in /Users/mdancho/opt/anaconda3/envs/py3.6/lib/python3.6/site-packages/scipy/special/_ufuncs.cpython-36m-darwin.so
```

__Solution:__

Downgrade Scipy from v1.4.1 to v1.4.0

`conda install -c conda-forge scipy=1.4.0 -n py3.6`

### Saving TensorFlow Models using H5

TensorFlow saves models in the HDF5 format. You will need `pytables` to save and load the HDF5 data. To install, use:

`conda install -c anaconda pytables -n py3.6`


# Data

Gold prices - Comparable to [Deep Learning in Finance](https://towardsdatascience.com/deep-learning-in-finance-9e088cb17c03).

## Quandl Data

```{r}
Quandl.api_key("sybXQ6yprb8MgJ6Ysf45")
#quandl_api_key(config::get("quandl", "../config.yml"))

gold_spot_prices_tbl <- tidyquant::tq_get(
    "LBMA/GOLD", 
    get  = "quandl", 
    from = "1985-01-01", 
    to   = "2017-01-01") %>%
    clean_names()

gold_spot_prices_tbl 
```

```{r}
# write_rds(gold_spot_prices_tbl, "data/gold_spot_prices.rds")
```


## Visualize the data

```{r}
gold_spot_prices_tbl <- read_rds("data/gold_spot_prices.rds")
```


```{r}
gold_spot_prices_tbl %>%
    plot_time_series(date, usd_pm, .smooth = F, .title = "Gold Spot Prices")
```

## Summarize Monthly

```{r}
gold_monthly_tbl <- gold_spot_prices_tbl %>%
    select(date, usd_pm) %>%
    rename(gold_price = usd_pm) %>%
    summarise_by_time(date, .by = "month", gold_price = mean(gold_price, na.rm = TRUE)) 

gold_monthly_tbl
```

```{r}
gold_monthly_tbl %>%
    plot_time_series(date, gold_price, .smooth = F)
```

```{r}
# write_rds(gold_monthly_tbl, "data/gold_monthly_tbl.rds")
```


# Preprocessing

```{r}
gold_monthly_tbl <- read_rds("data/gold_monthly_tbl.rds")
```


```{r}
train_tbl <- gold_monthly_tbl %>% filter_by_time(date, "start", "2010")
test_tbl  <- gold_monthly_tbl %>% filter_by_time(date, "2011", "2011") 
```


```{r, paged.print = FALSE}
recipe_spec <- recipe(~ ., data = train_tbl) %>%
    step_mutate(pct_change =  (gold_price - lag(gold_price)) / lag(gold_price)) %>%
    step_normalize(contains("pct_change")) %>%
    step_meanimpute(pct_change)

recipe_spec_prep <- recipe_spec %>% prep() 

recipe_spec_prep
```

```{r}
recipe_spec_prep %>% tidy()
```

```{r}
recipe_spec_prep %>% tidy(2)
```


```{r}
train_processed_tbl <- recipe_spec_prep %>% juice()
train_processed_tbl
```

## Return Time Plot

```{r}
train_processed_tbl %>%
    plot_time_series(date, pct_change)
```

## Return Autcorrelation

```{r}
train_processed_tbl %>% plot_acf_diagnostics(date, pct_change)
```

## Return Seasonality

```{r}
train_processed_tbl %>%
    plot_seasonal_diagnostics(date, pct_change)
```

```{r}
# write_rds(train_processed_tbl, "data/train_processed_tbl.rds")
train_processed_tbl <- read_rds("data/train_processed_tbl.rds")

# recipe_spec_prep %>% write_rds("data/recipe_spec_prep.rds")
recipe_spec_prep <- read_rds("data/recipe_spec_prep.rds")
```



# LSTM 

```{python}
import sys
import os
import time
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from datetime import datetime

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Input
from tensorflow.keras.layers import LSTM

from tensorflow.keras.models import load_model
```

```{python}
tf.random.set_seed(seed=5)
```


## Data Setup

```{python}
df = r.train_processed_tbl
df
```


```{python}
target = np.asarray(df['pct_change'])
target = np.atleast_2d(target)
target = target.T
        
target.shape
```

```{python}
# 12-month lookback period
look_back = 12

X = np.atleast_3d(
    np.array(
        [target[start:start + look_back] for start in range(0, target.shape[0] - look_back)]
    )
)

y = target[look_back:]
```

```{python}
X.shape
```

```{python}
y.shape
```

## LSTM Model

```{python}
model = Sequential()

model.add(Input(shape=(12,1)))
model.add(LSTM(64, return_sequences=True, activation='tanh'))
model.add(LSTM(12, return_sequences=False, activation='relu'))

model.add(Dense(1))

model.add(Activation(activation='linear'))

model.compile(optimizer="rmsprop", loss="mae")

```

```{python}
model.summary()
```


## Model Training (Takes a Minute)

- Epochs: Try 100 vs 1000 - See the differences

```{python, results="hide"}
model.fit(X, y, epochs=1000, batch_size=80, verbose=1, shuffle=False)
```

## Make Predictions (In-Sample)

```{python}
in_sample_predictions = model.predict(X)
in_sample_predictions[0][0]
```

```{python}
in_sample_predictions.shape
```

```{python}
predictions = in_sample_predictions.flatten()
predictions.shape
```



## Residual Analysis


```{r}
train_processed_tbl %>%
    add_column(pred = c(rep(NA, 12), py$predictions)) %>%
    ggplot(aes(pred, pct_change)) +
    geom_abline(slope = 1) +
    geom_point(alpha = 0.8) +
    geom_smooth(method = "lm", se = FALSE)
```





# Forecast (Future Predictions)

## Data Preparation

```{r}
new_data <- train_processed_tbl %>%
    future_frame(.length_out = 12) %>%
    bind_rows(train_processed_tbl, .) %>%
    replace_na(replace = list(pct_change = 0))

new_data %>% tail(14)
```

```{python}
def prepare_data(data, column = 'pct_change', look_back = 12):
    
    target = np.asarray(data[column])
    target = np.atleast_2d(target)

    if target.shape[0] == 1:
        target = target.T
        
    X = np.atleast_3d(
        np.array(
            [target[start:start + look_back] for start in range(0, target.shape[0] - look_back)]
        )
    )
    
    y = target[look_back:]
    
    return X, y
```


```{python}
X_new, y_new = prepare_data(r.new_data)

X_new.shape
```

## Make Prediction

```{python}
def predict_model(model, new_data):
    predictions_2d = model.predict(new_data)
    return predictions_2d.flatten()
```

```{python}
predictions_new = predict_model(model, new_data = X_new)
predictions_new.shape
```

## Residual Analysis

```{r}
forecast_tbl <- recipe_spec_prep %>% 
    bake(new_data = gold_monthly_tbl) %>%
    
    # Change this based on time-window being investigated
    filter_by_time(date, "start", "2011") %>%
    
    mutate(split = ifelse(date %in% train_tbl$date, "Training", "Test")) %>%
    add_column(predictions = c(rep(NA, 12), py$predictions_new))

forecast_tbl
```

```{r}
forecast_tbl %>%
    mutate(split = factor(split, levels = c("Training", "Test"))) %>%
    ggplot(aes(predictions, pct_change, color = split)) +
    geom_point(alpha = 0.8) +
    geom_abline(slope = 1) +
    scale_color_tq() +
    theme_tq()
```


## Visualize Forecast

```{r}
recipe_spec_prep %>% tidy()
```

```{r}
recipe_spec_prep %>% tidy(2)
```

```{r}
standardization_tbl <- recipe_spec_prep %>% tidy(2)
mean <- standardization_tbl %>% filter(statistic == "mean") %>% pull(value) 
sd   <- standardization_tbl %>% filter(statistic == "sd") %>% pull(value)

gold_price_start <- forecast_tbl %>%
    filter(split == "Training") %>%
    slice(n()) %>%
    pull(gold_price)

forecast_prepared_tbl <- forecast_tbl %>%
    filter(split == "Test") %>%
    mutate(predictions     = standardize_inv_vec(predictions, mean = mean, sd = sd)) %>%
    mutate(gold_prediction = gold_price_start * cumprod(1 + predictions)) %>%
    bind_rows(forecast_tbl %>% filter(split == "Training")) %>%
    select(date, gold_price, gold_prediction, split) %>%
    pivot_longer(cols = c(gold_price, gold_prediction)) %>%
    arrange(date, split, name) %>%
    drop_na() 

forecast_prepared_tbl %>%
    plot_time_series(date, value, .color_var = name, .smooth = F, .plotly_slider = T)
    
```

## Final Accuracy

```{r}
forecast_prepared_tbl %>%
    filter(split == "Test") %>%
    pivot_wider(names_from = name, values_from = value) %>%
    group_by(split) %>%
    summarize(
        mae   = mae_vec(gold_price, gold_prediction),
        rmse  = rmse_vec(gold_price, gold_prediction),
        mape  = mape_vec(gold_price, gold_prediction),
        smape = smape_vec(gold_price, gold_prediction)
    ) %>%
    ungroup()
```

# Save Model

```{python}
model.save("models/temp_1.h5")
```

```{python}
temp_1 = load_model("models/temp_1.h5")
```

```{python}
temp_1.summary()
```

# BONUS 1 - Sourcing Python Scripts in Shiny

Follow our workflow in R using Python under the hood!!! 

## Step 1 - Prepare data for TensorFlow LSTM

`prepare_data()`

```{r}
source_python("py/01_prepare_data.py")

training_data <- new_data %>% drop_na()

data_prepared_list <- prepare_data(training_data, column = "pct_change", look_back = 12L)

glimpse(data_prepared_list)
```

## Step 2 - TF LSTM Model the Prepared Data

`tensorflow_lstm()` 

```{r}
source_python("py/02_tensorflow_model.py")

X = data_prepared_list[[1]]
y = data_prepared_list[[2]]

tensorflow_lstm(X, y, epochs = 10L, activation_1 = "relu", loss = "mae")
```

## Step 3 - Predict New Data

`predict_lstm()`

- Loads model saved as `models/app_model.h5`
- Makes prediction from the TF Model

```{r}
source_python("py/03_model_prediction.py")

new_data_prepared_list <- prepare_data(new_data, "pct_change", look_back = 12L)

predictions <- predict_lstm(new_data_prepared_list[[1]])

glimpse(predictions)
```

```{r}
source_python("py/04_print_model_summary.py")

print_model_summary()
```




# BONUS 2 - Shiny App

- Automated TensorFlow Forecasting
- Explore effect of modeling parameter adjustments

# Next Steps

- __Energy Demand Forecasting__
- Multivariate Forecasting - We only did Univariate
- Validation Sets - We didn't use any cross validation
- Hyper Parameter Tuning
    - LSTM Steps
    - Optimizers
    - Model Fitting - Epochs, Batch Sizes
- Accuracy & Performance Improvement
- More Shiny Automation!

``` {python, eval=F}
# LSTM layer parameters - This is just one Layer!
tf.keras.layers.LSTM(
    units, activation='tanh', recurrent_activation='sigmoid', use_bias=True,
    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',
    bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,
    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,
    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,
    dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False,
    return_state=False, go_backwards=False, stateful=False, time_major=False,
    unroll=False, **kwargs
)
```

# References

- [Deep Learning for Finance: Deep Portfolios](https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.2209) (J.B. Heaton, N.G. Polson, and J.H. Witte)
- [Deep Learning in Finance](https://towardsdatascience.com/deep-learning-in-finance-9e088cb17c03) by Sonam Srivastava
- [Time Series Forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series#top_of_page) - TensorFlow Tutorials
